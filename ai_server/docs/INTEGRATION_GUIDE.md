# AI Server Integration Guide

## ðŸ“‹ ê°œìš”

AI ServerëŠ” Main Serverì™€ gRPC í†µì‹ ì„ í†µí•´ LLM(Qwen3-4B)ê³¼ Vision(YOLOv8n) ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

## ðŸ—ï¸ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         gRPC          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                 â”‚
â”‚   Main Server   â”‚   (Port: 50051)       â”‚   AI Server     â”‚
â”‚                 â”‚                       â”‚                 â”‚
â”‚  - Web API      â”‚                       â”‚  - LLM Service  â”‚
â”‚  - Task Mgmt    â”‚                       â”‚  - Vision Svc   â”‚
â”‚  - Fleet Mgmt   â”‚                       â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“ AI Server êµ¬ì¡°

```
ai_server/
â”œâ”€â”€ server.py                    # gRPC ì„œë²„ ë©”ì¸
â”œâ”€â”€ config.py                    # ì„¤ì •
â”œâ”€â”€ test_client.py              # í…ŒìŠ¤íŠ¸ í´ë¼ì´ì–¸íŠ¸
â”œâ”€â”€ start_ai_server.sh          # ì‹œìž‘ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ .env.example                # í™˜ê²½ ë³€ìˆ˜ ì˜ˆì‹œ
â”œâ”€â”€ README.md                   # AI Server ë¬¸ì„œ
â”œâ”€â”€ services/                   # AI ì„œë¹„ìŠ¤ ë ˆì´ì–´
â”‚   â”œâ”€â”€ llm_service.py         # LLM (Qwen3-4B)
â”‚   â””â”€â”€ vision_service.py      # Vision (YOLOv8n)
â””â”€â”€ grpc_impl/                 # gRPC êµ¬í˜„
    â”œâ”€â”€ ai_inference.proto
    â”œâ”€â”€ ai_inference_pb2.py
    â”œâ”€â”€ ai_inference_pb2_grpc.py
    â””â”€â”€ ai_inference_servicer.py
```

## ðŸš€ ì‹¤í–‰ ë°©ë²•

### 1. í™˜ê²½ ì„¤ì •

```bash
# .env íŒŒì¼ ìƒì„±
cp ai_server/.env.example .env

# ë˜ëŠ” ì§ì ‘ .env íŒŒì¼ì— ì¶”ê°€
cat >> .env << EOF

# AI Server Configuration
AI_INFERENCE_GRPC_HOST=0.0.0.0
AI_INFERENCE_GRPC_PORT=50051
LLM_MODEL_NAME=qwen3-4b
LLM_MODEL_PATH=./models/qwen3-4b
VISION_MODEL_NAME=yolov8n
VISION_MODEL_PATH=./models/yolov8n.pt
MAX_WORKERS=10
LOG_LEVEL=INFO
EOF
```

### 2. AI Server ì‹œìž‘

**ë°©ë²• 1: ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©**
```bash
./ai_server/start_ai_server.sh
```

**ë°©ë²• 2: ì§ì ‘ ì‹¤í–‰**
```bash
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
python3 -m ai_server.server
```

### 3. Main Server ì‹œìž‘

```bash
# ë³„ë„ í„°ë¯¸ë„ì—ì„œ
python3 -m uvicorn main_server.app:app --host 0.0.0.0 --port 8000
```

### 4. ì—°ê²° í…ŒìŠ¤íŠ¸

```bash
# AI Server í…ŒìŠ¤íŠ¸ í´ë¼ì´ì–¸íŠ¸ ì‹¤í–‰
python3 ai_server/test_client.py
```

## ðŸ”Œ í†µì‹  API

### DetectObjects (ê°ì²´ ì¸ì‹)

**Main Serverì—ì„œ ì‚¬ìš©:**
```python
from main_server.core_layer.ai_inference.grpc_inference_client import AIInferenceService

ai_service = AIInferenceService()
result = await ai_service.request_object_detection("image_123")

# ê²°ê³¼:
# {
#     "object_name": "person",
#     "confidence": 0.95,
#     "box": {"x": 100, "y": 150, "width": 200, "height": 300}
# }
```

### RecognizeFaces (ì–¼êµ´ ì¸ì‹)

**Main Serverì—ì„œ ì‚¬ìš©:**
```python
result = await ai_service.request_face_recognition("image_456")

# ê²°ê³¼:
# {
#     "person_type": "Employee",
#     "employee_id": "EMP001",
#     "confidence": 0.92
# }
```

### StreamInferenceResults (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°)

**Main Serverì—ì„œ ì‚¬ìš©:**
```python
# main_server/core_layer/ai_inference/grpc_inference_client.py ì—ì„œ
# ìŠ¤íŠ¸ë¦¬ë° ë©”ì„œë“œ êµ¬í˜„ í•„ìš”

async for result in ai_service.stream_inference_results():
    print(f"Robot: {result['robot_id']}, Result: {result}")
```

## ðŸ”§ ê°œë°œ ìƒíƒœ

### âœ… ì™„ë£Œëœ ê¸°ëŠ¥
- gRPC ì„œë²„ êµ¬ì¡°
- Main Serverì™€ í†µì‹  ì¸í„°íŽ˜ì´ìŠ¤
- LLM Service ë ˆì´ì–´ (ìŠ¤í…)
- Vision Service ë ˆì´ì–´ (ìŠ¤í…)
- ê¸°ë³¸ ì—ëŸ¬ í•¸ë“¤ë§
- ë¡œê¹… ì‹œìŠ¤í…œ

### ðŸš§ í–¥í›„ ìž‘ì—…
- [ ] Qwen3-4B ëª¨ë¸ í†µí•©
- [ ] YOLOv8n ëª¨ë¸ í†µí•©
- [ ] ì´ë¯¸ì§€ ë°ì´í„° ì „ì†¡ ë©”ì»¤ë‹ˆì¦˜
- [ ] ëª¨ë¸ ìºì‹± ë° ìµœì í™”
- [ ] ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›
- [ ] ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

## ðŸ“ ê°œë°œ ê°€ì´ë“œ

### ì‹¤ì œ LLM ëª¨ë¸ í†µí•© ì‹œ

[ai_server/services/llm_service.py](ai_server/services/llm_service.py) ìˆ˜ì •:

```python
def initialize(self):
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
    self.model = AutoModelForCausalLM.from_pretrained(
        self.model_path,
        device_map="auto",
        torch_dtype=torch.float16
    )
    logger.info("Qwen3-4B ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
```

### ì‹¤ì œ Vision ëª¨ë¸ í†µí•© ì‹œ

[ai_server/services/vision_service.py](ai_server/services/vision_service.py) ìˆ˜ì •:

```python
def initialize(self):
    from ultralytics import YOLO
    
    self.model = YOLO(self.model_path)
    logger.info("YOLOv8n ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

def detect_objects(self, image_id: str):
    # ì´ë¯¸ì§€ ë¡œë“œ
    image = load_image(image_id)
    
    # YOLO ì¶”ë¡ 
    results = self.model(image)
    
    # ê²°ê³¼ íŒŒì‹±
    for result in results:
        boxes = result.boxes
        # ... ì²˜ë¦¬
```

## ðŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. gRPC ì—°ê²° ì‹¤íŒ¨
```bash
# AI Server ìƒíƒœ í™•ì¸
ps aux | grep "ai_server.server"

# í¬íŠ¸ í™•ì¸
netstat -tuln | grep 50051
```

### 2. Import ì˜¤ë¥˜
```bash
# PYTHONPATH ì„¤ì • í™•ì¸
echo $PYTHONPATH

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ê°€ í¬í•¨ë˜ì–´ì•¼ í•¨
export PYTHONPATH="${PYTHONPATH}:/home/dh/dev_ws/git_ws/ros-repo-1"
```

### 3. ë¡œê·¸ í™•ì¸
AI ServerëŠ” ê¸°ë³¸ì ìœ¼ë¡œ INFO ë ˆë²¨ë¡œ ë¡œê·¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤:
```bash
# ë” ìžì„¸í•œ ë¡œê·¸ë¥¼ ì›í•˜ë©´ .env ìˆ˜ì •
LOG_LEVEL=DEBUG
```

## ðŸ“š ì°¸ê³  ìžë£Œ

- [AI Server README](ai_server/README.md)
- [gRPC Python Documentation](https://grpc.io/docs/languages/python/)
- [Protocol Buffers](https://developers.google.com/protocol-buffers)
- Main Server gRPC Client: [main_server/core_layer/ai_inference/grpc_inference_client.py](main_server/core_layer/ai_inference/grpc_inference_client.py)

## ðŸ“ž ì—°ë½ì²˜

ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ì´ìŠˆë¥¼ ë“±ë¡í•˜ê±°ë‚˜ ê°œë°œíŒ€ì— ë¬¸ì˜í•˜ì„¸ìš”.
