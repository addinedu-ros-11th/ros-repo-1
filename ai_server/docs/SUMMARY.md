# AI Server - ë¶„ë¦¬ ì•„í‚¤í…ì²˜ ì™„ë£Œ! âœ…

## ğŸ‰ êµ¬í˜„ ì™„ë£Œ ë‚´ìš©

### âœ… LLMê³¼ Vision ì„œë²„ ì™„ì „ ë¶„ë¦¬
- **LLM Server**: Port 50051 (ìì—°ì–´ ì²˜ë¦¬, ëŒ€í™”, ì˜ë„ ë¶„ì„)
- **Vision Server**: Port 50052 (ê°ì²´ ì¸ì‹, ì–¼êµ´ ì¸ì‹)

## ğŸ“Š ìµœì¢… ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Main Server    â”‚ (Port 8000)
â”‚  - Web API      â”‚
â”‚  - Task Mgmt    â”‚
â”‚  - Fleet Mgmt   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”˜
     â”‚         â”‚
     â”‚         â”‚ gRPC
     â”‚         â–¼
     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚    â”‚  Vision Server         â”‚ Port 50052
     â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
     â”‚    â”‚  â”‚ Vision Service   â”‚  â”‚
     â”‚    â”‚  â”‚ - YOLOv8n        â”‚  â”‚
     â”‚    â”‚  â”‚ - ê°ì²´ ì¸ì‹      â”‚  â”‚
     â”‚    â”‚  â”‚ - ì–¼êµ´ ì¸ì‹      â”‚  â”‚
     â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
     â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ gRPC
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Server            â”‚ Port 50051
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ LLM Service      â”‚  â”‚
â”‚  â”‚ - Qwen3-4B       â”‚  â”‚
â”‚  â”‚ - í…ìŠ¤íŠ¸ ìƒì„±    â”‚  â”‚
â”‚  â”‚ - ëŒ€í™”           â”‚  â”‚
â”‚  â”‚ - ì˜ë„ ë¶„ì„      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ëª¨ë“  ì„œë²„ ì‹œì‘
```bash
./ai_server/start_all_servers.sh
```

### 2. ê°œë³„ ì„œë²„ ì‹œì‘
```bash
# í„°ë¯¸ë„ 1: LLM
./ai_server/start_llm_server.sh

# í„°ë¯¸ë„ 2: Vision  
./ai_server/start_vision_server.sh
```

### 3. í…ŒìŠ¤íŠ¸
```bash
# ëª¨ë“  ì„œë²„ í…ŒìŠ¤íŠ¸
python3 ai_server/test_client_separated.py

# LLMë§Œ í…ŒìŠ¤íŠ¸
python3 ai_server/test_client_separated.py llm

# Visionë§Œ í…ŒìŠ¤íŠ¸
python3 ai_server/test_client_separated.py vision
```

## ğŸ’» Main Serverì—ì„œ ì‚¬ìš©

```python
from main_server.core_layer.ai_inference.llm_client import LLMClient
from main_server.core_layer.ai_inference.vision_client import VisionClient

# LLM ì‚¬ìš©
llm = LLMClient()
text = await llm.generate_text("ì•ˆë…•í•˜ì„¸ìš”")
response = await llm.chat([{"role": "user", "content": "Hi"}])
intent = await llm.analyze_intent("íšŒì˜ì‹¤ ì˜ˆì•½")

# Vision ì‚¬ìš©
vision = VisionClient()
obj = await vision.detect_objects("image_123")
face = await vision.recognize_faces("image_456")
objs = await vision.detect_multiple_objects("image_789")
```

## ğŸ“ í•µì‹¬ íŒŒì¼

### AI Server
- `llm_server.py` - LLM ì„œë²„ ë©”ì¸
- `vision_server.py` - Vision ì„œë²„ ë©”ì¸
- `grpc_impl/llm_servicer.py` - LLM gRPC êµ¬í˜„
- `grpc_impl/vision_servicer.py` - Vision gRPC êµ¬í˜„
- `grpc_impl/ai_services.proto` - ë¶„ë¦¬ëœ í”„ë¡œí† ì½œ ì •ì˜

### Main Server
- `core_layer/ai_inference/llm_client.py` - LLM í´ë¼ì´ì–¸íŠ¸
- `core_layer/ai_inference/vision_client.py` - Vision í´ë¼ì´ì–¸íŠ¸

### ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸
- `start_llm_server.sh` - LLM ì„œë²„ ì‹¤í–‰
- `start_vision_server.sh` - Vision ì„œë²„ ì‹¤í–‰
- `start_all_servers.sh` - ëª¨ë“  ì„œë²„ ì‹¤í–‰

### ë¬¸ì„œ
- `README_SEPARATED.md` - ë¶„ë¦¬ ì•„í‚¤í…ì²˜ ìƒì„¸ ê°€ì´ë“œ
- `ARCHITECTURE_COMPARISON.md` - í†µí•© vs ë¶„ë¦¬ ë¹„êµ
- `INTEGRATION_GUIDE.md` - í†µí•© ê°€ì´ë“œ

## ğŸ”§ í™˜ê²½ ì„¤ì •

```bash
# .env íŒŒì¼
LLM_GRPC_HOST=0.0.0.0
LLM_GRPC_PORT=50051
VISION_GRPC_HOST=0.0.0.0
VISION_GRPC_PORT=50052

LLM_MODEL_NAME=qwen3-4b
LLM_MODEL_PATH=./models/qwen3-4b
VISION_MODEL_NAME=yolov8n
VISION_MODEL_PATH=./models/yolov8n.pt

MAX_WORKERS=10
LOG_LEVEL=INFO
```

## ğŸ’¡ ì™œ ë¶„ë¦¬í–ˆëŠ”ê°€?

### 1. ë…ë¦½ì  ìŠ¤ì¼€ì¼ë§
```bash
# Visionì€ ì¹´ë©”ë¼ ëŒ€ìˆ˜ë§Œí¼ í™•ì¥
Vision Server Ã— 3 (Port 50052, 50053, 50054)

# LLMì€ ëŒ€í™” ë¹ˆë„ì— ë§ê²Œ ì¡°ì ˆ
LLM Server Ã— 1 (Port 50051)
```

### 2. ì¥ì•  ê²©ë¦¬
- Vision ì„œë²„ ì¥ì•  â†’ LLMìœ¼ë¡œ ìŒì„± ëŒ€í™”ëŠ” ê°€ëŠ¥
- LLM ì„œë²„ ì¥ì•  â†’ Visionìœ¼ë¡œ ê°ì²´ ì¸ì‹ì€ ê°€ëŠ¥

### 3. í•˜ë“œì›¨ì–´ ìµœì í™”
```
LLM Server  â†’ CPU/ë©”ëª¨ë¦¬ ì§‘ì•½ â†’ ê³ ë©”ëª¨ë¦¬ ì„œë²„
Vision Server â†’ GPU ì§‘ì•½ â†’ GPU íƒ‘ì¬ ì„œë²„
```

### 4. ë…ë¦½ ë°°í¬
```bash
# Visionë§Œ ì—…ë°ì´íŠ¸
git pull origin vision-update
./start_vision_server.sh

# LLMì€ ê¸°ì¡´ ë²„ì „ ìœ ì§€
```

## ğŸ“Š í¬íŠ¸ í• ë‹¹

| ì„œë¹„ìŠ¤ | í¬íŠ¸ | í”„ë¡œí† ì½œ | ìš©ë„ |
|--------|------|---------|------|
| Main Server | 8000 | HTTP | REST API, WebSocket |
| LLM Server | 50051 | gRPC | ìì—°ì–´ ì²˜ë¦¬ |
| Vision Server | 50052 | gRPC | ì»´í“¨í„° ë¹„ì „ |

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. **Proto ì¬ìƒì„±**: `ai_services.proto` â†’ Python ì½”ë“œ
2. **ì‹¤ì œ ëª¨ë¸ í†µí•©**: Qwen3-4B, YOLOv8n
3. **ë¡œë“œ ë°¸ëŸ°ì‹±**: ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ë¶„ì‚°
4. **ëª¨ë‹ˆí„°ë§**: Prometheus + Grafana
5. **CI/CD**: ë…ë¦½ ë°°í¬ íŒŒì´í”„ë¼ì¸

## ğŸ“š ì°¸ê³  ë¬¸ì„œ

- [README_SEPARATED.md](README_SEPARATED.md) - ìƒì„¸ ì‚¬ìš©ë²•
- [ARCHITECTURE_COMPARISON.md](ARCHITECTURE_COMPARISON.md) - ì•„í‚¤í…ì²˜ ë¹„êµ
- [INTEGRATION_GUIDE.md](INTEGRATION_GUIDE.md) - í†µí•© ê°€ì´ë“œ

## âœ¨ ìš”ì•½

- âœ… LLMê³¼ Vision ì„œë²„ ì™„ì „ ë¶„ë¦¬
- âœ… ë…ë¦½ì  í¬íŠ¸ (50051, 50052)
- âœ… Main Server í´ë¼ì´ì–¸íŠ¸ ë¶„ë¦¬
- âœ… ê°œë³„/í†µí•© ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸
- âœ… ìƒì„¸ ë¬¸ì„œ ì™„ë¹„
- âœ… í…ŒìŠ¤íŠ¸ í´ë¼ì´ì–¸íŠ¸

**ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜ í™˜ê²½ì— ìµœì í™”ëœ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ ì™„ì„±!** ğŸ‰
